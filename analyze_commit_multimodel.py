#!/usr/bin/env python3
"""
Enhanced Git Commit Analyzer with Multi-Model AI Analysis
Provides detailed scoring tables and commit history tracking
"""

import os
import sys
import json
import subprocess
import re
from datetime import datetime
from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass, asdict
from concurrent.futures import ThreadPoolExecutor
import time
from dotenv import load_dotenv
from openai import OpenAI
from anthropic import Anthropic
import google.generativeai as genai

load_dotenv()

@dataclass
class ModelScore:
    model_name: str
    provider: str
    code_quality: float  # 1-5
    dev_level: float     # 1-3 (1=junior, 2=mid, 3=senior)
    complexity: float    # 1-5
    estimated_hours: float
    ai_percentage: float  # 0-100% of code written by AI
    estimated_hours_with_ai: float  # Hours if developer used AI tools
    reasoning: str
    response_time: float

@dataclass
class CommitAnalysis:
    commit_hash: str
    commit_message: str
    timestamp: str
    user: str
    project: str
    file_changes: int
    lines_added: int
    lines_deleted: int
    model_scores: List[ModelScore]
    average_code_quality: float
    average_dev_level: float
    average_complexity: float
    average_estimated_hours: float
    average_ai_percentage: float
    average_estimated_hours_with_ai: float

class AIModels:
    def __init__(self):
        self.models = []
        self._setup_models()
    
    def _setup_models(self):
        # OpenAI setup
        openai_key = os.getenv("OPENAI_API_KEY")
        if openai_key and openai_key != "your_api_key_here":
            self.models.append({
                "name": "GPT-3.5 Turbo",
                "provider": "OpenAI",
                "client": OpenAI(),
                "type": "openai"
            })
        
        # Claude setup
        claude_key = os.getenv("CLAUDE_API_KEY")
        if claude_key and claude_key != "your_claude_api_key_here":
            self.models.append({
                "name": "Claude 3 Haiku",
                "provider": "Anthropic",
                "client": Anthropic(api_key=claude_key),
                "type": "claude"
            })
        
        # Gemini setup
        gemini_key = os.getenv("GEMINI_API_KEY")
        if gemini_key and gemini_key != "your_gemini_api_key_here":
            genai.configure(api_key=gemini_key)
            self.models.append({
                "name": "Gemini 1.5 Flash",
                "provider": "Google",
                "client": genai.GenerativeModel('gemini-1.5-flash'),
                "type": "gemini"
            })
        
        # Grok setup
        grok_key = os.getenv("GROK_API_KEY")
        if grok_key and grok_key != "your_grok_api_key_here":
            self.models.append({
                "name": "Grok 3",
                "provider": "xAI",
                "client": OpenAI(api_key=grok_key, base_url="https://api.x.ai/v1"),
                "type": "grok"
            })
    
    def analyze_commit(self, diff_content: str, commit_info: Dict) -> List[ModelScore]:
        prompt = f"""Analyze this git commit and provide scores on a scale as specified:

Commit: {commit_info['message']}
Author: {commit_info['author']}
Files changed: {commit_info['files_changed']}
Lines added: {commit_info['lines_added']}
Lines deleted: {commit_info['lines_deleted']}

Diff:
{diff_content[:2000]}  # Truncate for API limits

Please score the following metrics using DECIMAL PRECISION (e.g., 3.7, 2.3, 4.5):
1. Code Quality (1.0-5.0): Use decimals for nuanced scoring. 1.0=Poor, 2.0=Below Average, 3.0=Average, 4.0=Good, 5.0=Excellent
2. Developer Level (1.0-3.0): Use decimals. 1.0-1.5=Junior, 1.6-2.5=Mid-level, 2.6-3.0=Senior
3. Code Complexity (1.0-5.0): Use decimals. 1.0=Very Simple, 2.0=Simple, 3.0=Moderate, 4.0=Complex, 5.0=Very Complex
4. Estimated Development Time (in hours): Use decimals for precision
5. AI Code Percentage (0-100): Estimate what percentage of this code was likely written/generated by AI tools
6. Estimated Hours with AI: How many hours it would take if the developer used AI assistance

IMPORTANT: Provide precise decimal scores, not whole numbers. Each model should have its own unique perspective.

Respond ONLY in this JSON format:
{{
  "code_quality": 3.7,
  "dev_level": 2.3,
  "complexity": 3.4,
  "estimated_hours": 2.75,
  "ai_percentage": 45.5,
  "estimated_hours_with_ai": 1.25,
  "reasoning": "Brief explanation of your scoring"
}}"""

        if not self.models:
            return []
        
        results = []
        with ThreadPoolExecutor(max_workers=len(self.models)) as executor:
            futures = []
            for model in self.models:
                future = executor.submit(self._get_model_response, model, prompt)
                futures.append(future)
            
            for future in futures:
                result = future.result()
                if result:
                    results.append(result)
        
        return results
    
    def _get_model_response(self, model_info: Dict, prompt: str) -> Optional[ModelScore]:
        try:
            start_time = time.time()
            
            if model_info["type"] == "openai":
                response = model_info["client"].chat.completions.create(
                    model="gpt-3.5-turbo",
                    messages=[{"role": "user", "content": prompt}],
                    temperature=0.3,
                    max_tokens=500
                )
                result = response.choices[0].message.content
            
            elif model_info["type"] == "claude":
                response = model_info["client"].messages.create(
                    model="claude-3-haiku-20240307",
                    max_tokens=500,
                    temperature=0.3,
                    messages=[{"role": "user", "content": prompt}]
                )
                result = response.content[0].text
            
            elif model_info["type"] == "gemini":
                response = model_info["client"].generate_content(prompt)
                result = response.text
            
            elif model_info["type"] == "grok":
                response = model_info["client"].chat.completions.create(
                    model="grok-3",
                    messages=[{"role": "user", "content": prompt}],
                    temperature=0.3,
                    max_tokens=500
                )
                result = response.choices[0].message.content
            
            else:
                return None
            
            elapsed_time = time.time() - start_time
            
            # Parse JSON response
            try:
                # Extract JSON from response if it contains extra text
                json_start = result.find('{')
                json_end = result.rfind('}') + 1
                if json_start != -1 and json_end != 0:
                    json_str = result[json_start:json_end]
                    parsed = json.loads(json_str)
                else:
                    # Fallback parsing
                    parsed = json.loads(result)
                
                return ModelScore(
                    model_name=model_info["name"],
                    provider=model_info["provider"],
                    code_quality=float(parsed.get("code_quality", 3.0)),
                    dev_level=float(parsed.get("dev_level", 2.0)),
                    complexity=float(parsed.get("complexity", 3.0)),
                    estimated_hours=float(parsed.get("estimated_hours", 1.0)),
                    ai_percentage=float(parsed.get("ai_percentage", 0.0)),
                    estimated_hours_with_ai=float(parsed.get("estimated_hours_with_ai", 1.0)),
                    reasoning=parsed.get("reasoning", "No reasoning provided"),
                    response_time=elapsed_time
                )
            
            except (json.JSONDecodeError, ValueError) as e:
                # Fallback to default values if JSON parsing fails
                return ModelScore(
                    model_name=model_info["name"],
                    provider=model_info["provider"],
                    code_quality=3.0,
                    dev_level=2.0,
                    complexity=3.0,
                    estimated_hours=1.0,
                    ai_percentage=0.0,
                    estimated_hours_with_ai=1.0,
                    reasoning=f"JSON parsing error: {str(e)}",
                    response_time=elapsed_time
                )
        
        except Exception as e:
            return ModelScore(
                model_name=model_info["name"],
                provider=model_info["provider"],
                code_quality=0.0,
                dev_level=0.0,
                complexity=0.0,
                estimated_hours=0.0,
                ai_percentage=0.0,
                estimated_hours_with_ai=0.0,
                reasoning=f"Error: {str(e)}",
                response_time=0.0
            )

class CommitDatabase:
    def __init__(self, db_file="commit_analysis_history.json"):
        self.db_file = db_file
        self.history = self._load_history()
    
    def _load_history(self) -> List[Dict]:
        if os.path.exists(self.db_file):
            try:
                with open(self.db_file, 'r') as f:
                    return json.load(f)
            except (json.JSONDecodeError, FileNotFoundError):
                return []
        return []
    
    def save_analysis(self, analysis: CommitAnalysis):
        # Convert to dict and add to history
        analysis_dict = asdict(analysis)
        self.history.append(analysis_dict)
        
        # Save to file
        with open(self.db_file, 'w') as f:
            json.dump(self.history, f, indent=2)
    
    def get_history(self) -> List[Dict]:
        return self.history

def get_commit_info(commit_hash: str = "HEAD") -> Tuple[str, Dict]:
    """Get commit diff and info"""
    try:
        # Get commit info
        info_result = subprocess.run(
            ['git', 'show', '--format=%H|%s|%an|%ad', '--date=iso', '--no-patch', commit_hash],
            capture_output=True, text=True, check=True
        )
        
        # Get diff
        diff_result = subprocess.run(
            ['git', 'show', '--format=', '--stat', commit_hash],
            capture_output=True, text=True, check=True
        )
        
        # Get actual diff content
        diff_content = subprocess.run(
            ['git', 'show', '--format=', commit_hash],
            capture_output=True, text=True, check=True
        ).stdout
        
        # Parse commit info
        info_line = info_result.stdout.strip()
        hash_val, message, author, date = info_line.split('|', 3)
        
        # Parse stats
        stat_lines = diff_result.stdout.strip().split('\n')
        files_changed = 0
        lines_added = 0
        lines_deleted = 0
        
        for line in stat_lines:
            if 'changed' in line and ('insertion' in line or 'deletion' in line):
                # Parse format: "X files changed, Y insertions(+), Z deletions(-)"
                parts = line.strip().split()
                for i, part in enumerate(parts):
                    if part == 'files' and i > 0:
                        files_changed = int(parts[i-1])
                    elif 'insertion' in part and i > 0:
                        lines_added = int(parts[i-1])
                    elif 'deletion' in part and i > 0:
                        lines_deleted = int(parts[i-1])
        
        commit_info = {
            'hash': hash_val,
            'message': message,
            'author': author,
            'date': date,
            'files_changed': files_changed,
            'lines_added': lines_added,
            'lines_deleted': lines_deleted
        }
        
        return diff_content, commit_info
        
    except subprocess.CalledProcessError as e:
        print(f"Error getting commit info: {e}")
        sys.exit(1)

def print_model_scores_table(model_scores: List[ModelScore]):
    """Print detailed model scores table"""
    print("\n" + "="*150)
    print("📊 DETAILED MODEL ANALYSIS")
    print("="*150)
    
    # Header
    print(f"{'Model':<20} {'Provider':<10} {'Quality':<8} {'Dev Lvl':<10} {'Complex':<8} {'Hours':<7} {'AI %':<7} {'AI Hrs':<7} {'Time(s)':<7}")
    print("-" * 150)
    
    # Data rows
    for score in model_scores:
        dev_level_str = f"{score.dev_level:.1f} ({'Jr' if score.dev_level <= 1.5 else 'Mid' if score.dev_level <= 2.5 else 'Sr'})"
        print(f"{score.model_name:<20} {score.provider:<10} {score.code_quality:<8.1f} {dev_level_str:<10} {score.complexity:<8.1f} {score.estimated_hours:<7.1f} {score.ai_percentage:<7.1f} {score.estimated_hours_with_ai:<7.1f} {score.response_time:<7.2f}")
    
    print("\n📝 Model Reasoning:")
    for i, score in enumerate(model_scores, 1):
        print(f"{i}. {score.provider} - {score.model_name}:")
        print(f"   {score.reasoning}\n")

def print_commit_history_table(history: List[Dict]):
    """Print commit history summary table"""
    if not history:
        print("\n📈 COMMIT HISTORY: No previous commits analyzed")
        return
    
    print("\n" + "="*140)
    print("📈 COMMIT HISTORY SUMMARY")
    print("="*140)
    
    # Header
    print(f"{'Date':<12} {'Hash':<8} {'Author':<15} {'Avg Quality':<11} {'Avg Dev Lvl':<11} {'Avg Complex':<11} {'Avg Hours':<10} {'Message':<50}")
    print("-" * 140)
    
    # Data rows (show last 10)
    recent_history = history[-10:] if len(history) > 10 else history
    
    for commit in recent_history:
        date_str = commit['timestamp'][:10]  # YYYY-MM-DD
        hash_short = commit['commit_hash'][:8]
        author = commit['user'][:14]
        message = commit['commit_message'][:49]
        
        print(f"{date_str:<12} {hash_short:<8} {author:<15} {commit['average_code_quality']:<11.1f} {commit['average_dev_level']:<11.1f} {commit['average_complexity']:<11.1f} {commit['average_estimated_hours']:<10.1f} {message:<50}")
    
    if len(history) > 10:
        print(f"\n... and {len(history) - 10} more commits")

def main():
    """Main entry point"""
    # Get commit hash from arguments or use HEAD
    commit_hash = sys.argv[1] if len(sys.argv) > 1 else "HEAD"
    user = sys.argv[2] if len(sys.argv) > 2 else "unknown"
    project = sys.argv[3] if len(sys.argv) > 3 else "unknown"
    
    # Check if we're in a git repository
    try:
        subprocess.run(['git', 'rev-parse', '--git-dir'], check=True, capture_output=True)
    except subprocess.CalledProcessError:
        print("Error: Not in a git repository")
        sys.exit(1)
    
    print(f"🚀 Enhanced Multi-Model Commit Analyzer")
    print(f"📝 User: {user} | Project: {project}")
    print("="*80)
    
    # Get commit diff and info
    diff_content, commit_info = get_commit_info(commit_hash)
    
    if not diff_content.strip():
        print("No changes found in this commit")
        sys.exit(0)
    
    print(f"Analyzing commit: {commit_info['hash'][:8]} - {commit_info['message']}")
    print(f"Author: {commit_info['author']} | Date: {commit_info['date']}")
    print(f"Files: {commit_info['files_changed']} | +{commit_info['lines_added']} -{commit_info['lines_deleted']}")
    
    # Initialize AI models and analyze
    ai_models = AIModels()
    print(f"\n🤖 Analyzing with {len(ai_models.models)} AI models...")
    
    model_scores = ai_models.analyze_commit(diff_content, commit_info)
    
    if not model_scores:
        print("No AI models available. Please check your API keys in .env file.")
        sys.exit(1)
    
    # Calculate averages
    avg_quality = sum(score.code_quality for score in model_scores) / len(model_scores)
    avg_dev_level = sum(score.dev_level for score in model_scores) / len(model_scores)
    avg_complexity = sum(score.complexity for score in model_scores) / len(model_scores)
    avg_hours = sum(score.estimated_hours for score in model_scores) / len(model_scores)
    avg_ai_percentage = sum(score.ai_percentage for score in model_scores) / len(model_scores)
    avg_hours_with_ai = sum(score.estimated_hours_with_ai for score in model_scores) / len(model_scores)
    
    # Create analysis record
    analysis = CommitAnalysis(
        commit_hash=commit_info['hash'],
        commit_message=commit_info['message'],
        timestamp=datetime.now().isoformat(),
        user=user,
        project=project,
        file_changes=commit_info['files_changed'],
        lines_added=commit_info['lines_added'],
        lines_deleted=commit_info['lines_deleted'],
        model_scores=model_scores,
        average_code_quality=avg_quality,
        average_dev_level=avg_dev_level,
        average_complexity=avg_complexity,
        average_estimated_hours=avg_hours,
        average_ai_percentage=avg_ai_percentage,
        average_estimated_hours_with_ai=avg_hours_with_ai
    )
    
    # Print results
    print_model_scores_table(model_scores)
    
    print(f"\n📊 AVERAGE SCORES:")
    print(f"Code Quality: {avg_quality:.1f}/5")
    print(f"Developer Level: {avg_dev_level:.1f}/3 ({'Junior' if avg_dev_level <= 1.5 else 'Mid' if avg_dev_level <= 2.5 else 'Senior'})")
    print(f"Complexity: {avg_complexity:.1f}/5")
    print(f"Estimated Hours: {avg_hours:.1f}")
    print(f"AI Code Percentage: {avg_ai_percentage:.1f}%")
    print(f"Estimated Hours with AI: {avg_hours_with_ai:.1f}")
    print(f"Time Savings with AI: {avg_hours - avg_hours_with_ai:.1f} hours ({((avg_hours - avg_hours_with_ai) / avg_hours * 100):.0f}% reduction)")
    
    # Save to database and show history
    db = CommitDatabase()
    db.save_analysis(analysis)
    
    print_commit_history_table(db.get_history())
    
    print(f"\n✅ Analysis complete! Saved to {db.db_file}")

if __name__ == "__main__":
    main()